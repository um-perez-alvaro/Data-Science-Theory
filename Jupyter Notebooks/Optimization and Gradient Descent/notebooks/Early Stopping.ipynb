{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, **early stopping** is a form of **regularization** used to avoid overfitting.\n",
    "As soon as the test error rearches a minimum, we will stop Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate and plot a toy dataset\n",
    "m = 15\n",
    "x = -1 + 2*np.random.rand(m)\n",
    "y = 2*x+1 + 0.25*np.random.randn(m)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(x,y,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets\n",
    "x_train,y_train = x[:10],y[:10]\n",
    "x_test,y_test = x[10:],y[10:]\n",
    "m_train = len(y_train)\n",
    "m_test = len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfitted polynomial example\n",
    "degree = 10\n",
    "\n",
    "# build X matrices\n",
    "X_train = np.ones((m_train,degree+1))\n",
    "X_test = np.ones((m_test,degree+1))\n",
    "for i in range(degree):\n",
    "    X_train[:,i+1]=x_train**(i+1)\n",
    "    X_test[:,i+1]=x_test**(i+1)\n",
    "    \n",
    "    \n",
    "# fit a polynomial (to the training set)\n",
    "theta = np.linalg.solve(X_train.T.dot(X_train),X_train.T.dot(y_train))\n",
    "\n",
    "# plot data\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(x_train,y_train,'bo', label = 'training set')\n",
    "plt.plot(x_test,y_test,'ro', label = 'test set')\n",
    "\n",
    "# plot the fitted polynomial\n",
    "m_plot = 100\n",
    "x_plot = np.linspace(-1,1,m_plot) \n",
    "X_plot = np.ones((m_plot,degree+1))\n",
    "for i in range(degree):\n",
    "    X_plot[:,i+1]=x_plot**(i+1)\n",
    "y_plot = X_plot.dot(theta)\n",
    "plt.plot(x_plot,y_plot,'r-', label='fitted polynomial')\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "plt.title('Overfitted Model',fontsize = 15)\n",
    "plt.ylim(-10,10)\n",
    "\n",
    "# MSEs\n",
    "MSE_train = np.linalg.norm(y_train-X_train@theta)**2/len(x_train)\n",
    "MSE_test = np.linalg.norm(y_test-X_test@theta)**2/len(x_train)\n",
    "print('Training Set Mean Squared Error: '+str(MSE_train))\n",
    "print('Test Set Mean Squared Error: '+str(MSE_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linregression_GD(X,y,learning_rate, n_epochs = 100, return_MSE=False, test_data = None):\n",
    "    '''\n",
    "    linear regression with Gradient Descent\n",
    "    \n",
    "    INPUT: \n",
    "    - the matrix X\n",
    "    - the vector y\n",
    "    - learning rate\n",
    "    - epochs: number of Gradient Descent iterations (defualt 100)\n",
    "    - return_MSE: if True, it returs the mse at each iteration (default False)\n",
    "    - test_data: data (X_test,y_test) for monitoring overfitting\n",
    "    \n",
    "    OUTPUT:\n",
    "    - the vector theta\n",
    "    - MSE: error at each iteration\n",
    "    '''\n",
    "    m,n = X.shape # size of data set, number of features\n",
    "    theta = np.random.randn(n) # random initialization\n",
    "    \n",
    "    # initialize MSE vector (only if retur_MSE = True)\n",
    "    if return_MSE:\n",
    "        MSE = np.zeros(n_epochs)\n",
    "    \n",
    "    # initialize MSE_test vector (only if test_data not None)\n",
    "    if test_data:\n",
    "        X_test,y_test = test_data\n",
    "        m_test = len(y_test)\n",
    "        MSE_test = np.zeros(n_epochs)\n",
    "    \n",
    "    # gradient descent iterations\n",
    "    for epoch in range(n_epochs):\n",
    "        gradient = (2/m)*X.T.dot(X.dot(theta)-y) # gradient of the mse function\n",
    "        theta = theta - learning_rate*gradient # update the vector theta\n",
    "        # compute mean squared error (only if return_MSE = True)\n",
    "        if return_MSE:\n",
    "            MSE[epoch] = np.linalg.norm(y-X.dot(theta))**2/m\n",
    "        # compute mean squared error on test set (only if test_data is not None)\n",
    "        if test_data:\n",
    "            MSE_test[epoch] = np.linalg.norm(y_test-X_test.dot(theta))**2/m_test\n",
    "    \n",
    "    results = {}\n",
    "    results['coeff'] = theta\n",
    "    if return_MSE:\n",
    "        results['error'] = MSE\n",
    "    if test_data:\n",
    "        results['test_error'] = MSE_test\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "lr = 0.5 # learning rate\n",
    "epochs = 5000 # number of gradient descent iterations\n",
    "results = linregression_GD(X_train,\n",
    "                                              y_train, \n",
    "                                              n_epochs = epochs, \n",
    "                                              learning_rate = lr, \n",
    "                                              return_MSE = True,\n",
    "                                              test_data = (X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = results['coeff']\n",
    "MSE_train = results['error']\n",
    "MSE_test = results['test_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(MSE_train,'b', label = 'training set')\n",
    "plt.plot(MSE_test, 'r', label = 'test set')\n",
    "plt.legend(fontsize=15)\n",
    "plt.title('Mean Squared Error', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping of Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "epochs = 70\n",
    "results = linregression_GD(X_train,y_train,n_epochs=epochs, learning_rate = lr)\n",
    "theta = results['coeff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(x_train,y_train,'bo', label = 'training set')\n",
    "plt.plot(x_test,y_test,'ro', label = 'test set')\n",
    "\n",
    "# plot the fitted polynomial\n",
    "m_plot = 100\n",
    "x_plot = np.linspace(-1,1,m_plot) \n",
    "X_plot = np.ones((m_plot,degree+1))\n",
    "for i in range(degree):\n",
    "    X_plot[:,i+1]=x_plot**(i+1)\n",
    "y_plot = X_plot.dot(theta)\n",
    "plt.plot(x_plot,y_plot,'r-', label='fitted polynomial')\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "plt.ylim(-10,10)\n",
    "\n",
    "# MSEs\n",
    "MSE_train = np.linalg.norm(y_train-X_train@theta)**2/len(x_train)\n",
    "MSE_test = np.linalg.norm(y_test-X_test@theta)**2/len(x_train)\n",
    "print('Training Set Mean Squared Error: '+str(MSE_train))\n",
    "print('Test Set Mean Squared Error: '+str(MSE_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
